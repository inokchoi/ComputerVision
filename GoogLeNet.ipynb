{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogLeNet_incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inokchoi/ComputerVision/blob/main/GoogLeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OZdq-mad8Lo",
        "outputId": "0e2bfa5b-2bd8-49bc-e2d1-700d0c46a96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from tensorflow.keras.datasets.cifar10 import load_data\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVPsnVtCe7Rw"
      },
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4lVc9TYfJ17",
        "outputId": "dd505499-1828-4a25-e491-d1b79a7a33f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# (X_train, y_train), (X_test, y_test) = load_data()\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False, one_hot=True)\n",
        "X_train, y_train = mnist.train.images, mnist.train.labels\n",
        "X_valid, y_valid = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test = mnist.test.images, mnist.test.labels\n",
        "\n",
        "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
        "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "print(\"Validation Set: {} samples\".format(len(X_valid)))\n",
        "print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-a31e29a56efa>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Image Shape: (28, 28, 1)\n",
            "Training Set:   55000 samples\n",
            "Validation Set: 5000 samples\n",
            "Test Set:       10000 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOBmmNMLg86G"
      },
      "source": [
        "def conv2d(input, filters, kernel_size, strides=1, activation=tf.nn.relu, padding='SAME', name='conv'):\n",
        "    with tf.variable_scope(name, reuse=False):\n",
        "        out = tf.layers.conv2d(input, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                         activation=activation, name=name)\n",
        "    return out\n",
        "\n",
        "def dense(input, unit, activation=tf.nn.relu, name='dense'):\n",
        "    with tf.variable_scope(name, reuse=False):\n",
        "        out = tf.layers.dense(input, unit, activation=activation, name=name)\n",
        "    return out\n",
        "\n",
        "def max_pool(input, k, s, name='pool'):\n",
        "    out = tf.nn.max_pool(input, ksize=[1, k, k, 1], strides=[1, s, s, 1], padding='SAME', name=name)\n",
        "    return out\n",
        "\n",
        "def lrn(name, inputs, depth_radius=5, alpha=0.0001, beta=0.75):\n",
        "    with tf.variable_scope(name, reuse=False):\n",
        "        out = tf.nn.local_response_normalization(name=name, input=inputs, depth_radius=depth_radius, alpha=alpha, beta=beta)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCw3qaenhTJe"
      },
      "source": [
        "def inception(x, conv11_size, conv33_rsize, conv33_size, conv55_rsize, conv55_size, pool_size, name='incept'):\n",
        "    conv1_1 = conv2d(x, conv11_size, [1,1], [1,1], name='{}_1x1'.format(name))\n",
        "\n",
        "    conv3_3_red = conv2d(x, conv33_rsize, [1,1], [1,1], name='{}_3x3_red'.format(name))\n",
        "    conv3_3 = conv2d(conv3_3_red, conv33_size, [3,3], [1,1], name='{}_3x3'.format(name))\n",
        "\n",
        "    conv5_5_red = conv2d(x, conv55_rsize, [1,1], [1,1], name='{}_5x5_red'.format(name))\n",
        "    conv5_5 = conv2d(conv5_5_red, conv55_size, [5,5], [1,1], name='{}_5x5'.format(name))\n",
        "\n",
        "    pool = max_pool(x, 3, 1, name='{}_pool'.format(name))\n",
        "    pool_conv = conv2d(pool, pool_size, [1,1],[1,1], name='{}_pool_conv'.format(name))\n",
        "\n",
        "    out = tf.concat([conv1_1, conv3_3, conv5_5, pool_conv], axis=-1)\n",
        "\n",
        "    return out\n",
        "\n",
        "def googlenet(x):\n",
        "    conv1 = conv2d(x, 64, [7,7], [2,2], name='conv1')\n",
        "    pool1 = max_pool(conv1, 3, 2, name='pool1')\n",
        "    pool1_norm1 = lrn('pool1_norm1', pool1)\n",
        "\n",
        "    conv2 = conv2d(pool1_norm1, 64, [1,1], [1,1], name='conv2')\n",
        "    conv3 = conv2d(conv2, 192, [3,3], [1,1], name='conv3')\n",
        "    conv3_norm1 = lrn('conv3_norm1', conv3)\n",
        "    pool2 = max_pool(conv3_norm1, 3, 2, name='pool1')\n",
        "\n",
        "    inception1 = inception(pool2, 64,96,128,16,32,32, name='inception1')\n",
        "    inception2 = inception(inception1, 128,128,192,32,96,64, name='inception2')\n",
        "    pool3 = max_pool(inception2, 3, 2, name='pool3')\n",
        "\n",
        "    inception3 = inception(pool3, 192,96,208,16,48,64, name='inception3')\n",
        "    inception4 = inception(inception3, 160,112,224,24,64,64, name='inception4')\n",
        "    inception5 = inception(inception4, 128,128,256,24,64,64, name='inception5')\n",
        "    inception6 = inception(inception5, 112,144,288,32,64,64, name='inception6')\n",
        "    inception7 = inception(inception6, 256,160,320,32,128,128, name='inception7')\n",
        "    pool4 = max_pool(inception7, 3, 2, name='pool4')\n",
        "    \n",
        "    inception8 = inception(pool4, 256,160,320,32,128,128, name='inception8')\n",
        "    inception9 = inception(inception8, 384,192,384,48,128,128, name='inception9')\n",
        "\n",
        "    avg_pool1 = tf.nn.avg_pool(inception9, ksize=[1, 7, 7, 1], strides=[1, 1, 1, 1], padding='SAME', name='avg_pool1')\n",
        "    dropout1 = tf.nn.dropout(avg_pool1,0.6)\n",
        "    \n",
        "    dense1 = tf.reshape(dropout1, [-1, tf.size(inception9[0])])\n",
        "    dense2 = dense(dense1, 10, name='dense2')\n",
        "\n",
        "    return dense2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVmFx2RTiRtw",
        "outputId": "cf000a11-63fa-4e63-eba9-220d6d4cdadf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.00001\n",
        "batch_size = 64\n",
        "dropout = 0.8\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None, 10])\n",
        "# y_one_hot = tf.squeeze(tf.one_hot(y, 10), axis=1)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32, [])  # dropout (keep probability)\n",
        "x_resize = tf.image.resize_images(x, [64, 64])\n",
        "\n",
        "# Construct model\n",
        "pred = googlenet(x)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.initialize_all_variables()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-b14a244babec>:4: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-809e0194bbf9>:42: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-4-b14a244babec>:9: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7tHaaCpiYp1",
        "outputId": "28fa7219-bd11-4ff2-e2a3-9847367c7e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n",
        "                                      gpu_options=tf.GPUOptions(allow_growth=True, visible_device_list='0'))) as sess:\n",
        "    sess.run(init)\n",
        "    # Keep training until reach max iterations\n",
        "    print('Training...')\n",
        "    for i in range(epochs):\n",
        "        s = np.random.permutation(len(X_train))\n",
        "        X_train = X_train[s]\n",
        "        y_train = y_train[s]\n",
        "        loss_total, acc_total = 0, 0\n",
        "        for offset in range(0, len(X_train), batch_size):\n",
        "            end = offset + batch_size\n",
        "            batch_xs, batch_ys = X_train[offset:end], y_train[offset:end]\n",
        "            # Fit training using batch data\n",
        "            _, acc, loss = sess.run([optimizer, accuracy, cost], feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
        "\n",
        "            # Calculate batch accuracy\n",
        "            # Calculate batch loss\n",
        "            acc_total += acc\n",
        "            loss_total += loss\n",
        "        acc_total /= (len(X_train) / batch_size)\n",
        "        loss_total /= (len(X_train) / batch_size)\n",
        "        print(\"epoch: \" + str(i) + \", Training Loss= \" + \"{:.4f}\".format(\n",
        "            loss_total) + \", Training Accuracy= \" + \"{:.4f}\".format(acc_total))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "    saver.save(sess, './goolgenet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "epoch: 0, Training Loss= 1.5045, Training Accuracy= 0.4972\n",
            "epoch: 1, Training Loss= 0.7099, Training Accuracy= 0.7543\n",
            "epoch: 2, Training Loss= 0.5530, Training Accuracy= 0.8187\n",
            "epoch: 3, Training Loss= 0.4627, Training Accuracy= 0.8572\n",
            "epoch: 4, Training Loss= 0.3979, Training Accuracy= 0.8807\n",
            "epoch: 5, Training Loss= 0.3474, Training Accuracy= 0.8982\n",
            "epoch: 6, Training Loss= 0.3092, Training Accuracy= 0.9101\n",
            "epoch: 7, Training Loss= 0.2829, Training Accuracy= 0.9199\n",
            "epoch: 8, Training Loss= 0.2535, Training Accuracy= 0.9283\n",
            "epoch: 9, Training Loss= 0.2288, Training Accuracy= 0.9364\n",
            "epoch: 10, Training Loss= 0.2042, Training Accuracy= 0.9429\n",
            "epoch: 11, Training Loss= 0.1875, Training Accuracy= 0.9480\n",
            "epoch: 12, Training Loss= 0.1801, Training Accuracy= 0.9488\n",
            "epoch: 13, Training Loss= 0.1625, Training Accuracy= 0.9543\n",
            "epoch: 14, Training Loss= 0.1507, Training Accuracy= 0.9575\n",
            "epoch: 15, Training Loss= 0.1445, Training Accuracy= 0.9595\n",
            "epoch: 16, Training Loss= 0.1340, Training Accuracy= 0.9618\n",
            "epoch: 17, Training Loss= 0.1252, Training Accuracy= 0.9649\n",
            "epoch: 18, Training Loss= 0.1206, Training Accuracy= 0.9663\n",
            "epoch: 19, Training Loss= 0.1139, Training Accuracy= 0.9679\n",
            "epoch: 20, Training Loss= 0.1047, Training Accuracy= 0.9710\n",
            "epoch: 21, Training Loss= 0.0985, Training Accuracy= 0.9728\n",
            "epoch: 22, Training Loss= 0.0952, Training Accuracy= 0.9733\n",
            "epoch: 23, Training Loss= 0.0866, Training Accuracy= 0.9767\n",
            "epoch: 24, Training Loss= 0.0809, Training Accuracy= 0.9782\n",
            "epoch: 25, Training Loss= 0.0784, Training Accuracy= 0.9787\n",
            "epoch: 26, Training Loss= 0.0745, Training Accuracy= 0.9792\n",
            "epoch: 27, Training Loss= 0.0689, Training Accuracy= 0.9811\n",
            "epoch: 28, Training Loss= 0.0652, Training Accuracy= 0.9824\n",
            "epoch: 29, Training Loss= 0.0641, Training Accuracy= 0.9821\n",
            "epoch: 30, Training Loss= 0.0600, Training Accuracy= 0.9837\n",
            "epoch: 31, Training Loss= 0.0562, Training Accuracy= 0.9843\n",
            "epoch: 32, Training Loss= 0.0524, Training Accuracy= 0.9858\n",
            "epoch: 33, Training Loss= 0.0473, Training Accuracy= 0.9867\n",
            "epoch: 34, Training Loss= 0.0433, Training Accuracy= 0.9880\n",
            "epoch: 35, Training Loss= 0.0416, Training Accuracy= 0.9886\n",
            "epoch: 36, Training Loss= 0.0415, Training Accuracy= 0.9888\n",
            "epoch: 37, Training Loss= 0.0393, Training Accuracy= 0.9893\n",
            "epoch: 38, Training Loss= 0.0370, Training Accuracy= 0.9891\n",
            "epoch: 39, Training Loss= 0.0335, Training Accuracy= 0.9909\n",
            "epoch: 40, Training Loss= 0.0307, Training Accuracy= 0.9914\n",
            "epoch: 41, Training Loss= 0.0327, Training Accuracy= 0.9908\n",
            "epoch: 42, Training Loss= 0.0291, Training Accuracy= 0.9922\n",
            "epoch: 43, Training Loss= 0.0282, Training Accuracy= 0.9923\n",
            "epoch: 44, Training Loss= 0.0279, Training Accuracy= 0.9923\n",
            "epoch: 45, Training Loss= 0.0246, Training Accuracy= 0.9933\n",
            "epoch: 46, Training Loss= 0.0228, Training Accuracy= 0.9939\n",
            "epoch: 47, Training Loss= 0.0234, Training Accuracy= 0.9935\n",
            "epoch: 48, Training Loss= 0.0210, Training Accuracy= 0.9945\n",
            "epoch: 49, Training Loss= 0.0213, Training Accuracy= 0.9942\n",
            "epoch: 50, Training Loss= 0.0211, Training Accuracy= 0.9945\n",
            "epoch: 51, Training Loss= 0.0162, Training Accuracy= 0.9962\n",
            "epoch: 52, Training Loss= 0.0170, Training Accuracy= 0.9954\n",
            "epoch: 53, Training Loss= 0.0193, Training Accuracy= 0.9946\n",
            "epoch: 54, Training Loss= 0.0149, Training Accuracy= 0.9964\n",
            "epoch: 55, Training Loss= 0.0136, Training Accuracy= 0.9969\n",
            "epoch: 56, Training Loss= 0.0150, Training Accuracy= 0.9961\n",
            "epoch: 57, Training Loss= 0.0137, Training Accuracy= 0.9968\n",
            "epoch: 58, Training Loss= 0.0123, Training Accuracy= 0.9969\n",
            "epoch: 59, Training Loss= 0.0138, Training Accuracy= 0.9966\n",
            "epoch: 60, Training Loss= 0.0109, Training Accuracy= 0.9975\n",
            "epoch: 61, Training Loss= 0.0116, Training Accuracy= 0.9973\n",
            "epoch: 62, Training Loss= 0.0122, Training Accuracy= 0.9972\n",
            "epoch: 63, Training Loss= 0.0101, Training Accuracy= 0.9979\n",
            "epoch: 64, Training Loss= 0.0094, Training Accuracy= 0.9980\n",
            "epoch: 65, Training Loss= 0.0103, Training Accuracy= 0.9977\n",
            "epoch: 66, Training Loss= 0.0102, Training Accuracy= 0.9978\n",
            "epoch: 67, Training Loss= 0.0087, Training Accuracy= 0.9984\n",
            "epoch: 68, Training Loss= 0.0085, Training Accuracy= 0.9983\n",
            "epoch: 69, Training Loss= 0.0075, Training Accuracy= 0.9986\n",
            "epoch: 70, Training Loss= 0.0102, Training Accuracy= 0.9979\n",
            "epoch: 71, Training Loss= 0.0084, Training Accuracy= 0.9983\n",
            "epoch: 72, Training Loss= 0.0063, Training Accuracy= 0.9991\n",
            "epoch: 73, Training Loss= 0.0087, Training Accuracy= 0.9981\n",
            "epoch: 74, Training Loss= 0.0050, Training Accuracy= 0.9993\n",
            "epoch: 75, Training Loss= 0.0074, Training Accuracy= 0.9985\n",
            "epoch: 76, Training Loss= 0.0080, Training Accuracy= 0.9983\n",
            "epoch: 77, Training Loss= 0.0032, Training Accuracy= 0.9999\n",
            "epoch: 78, Training Loss= 0.0082, Training Accuracy= 0.9982\n",
            "epoch: 79, Training Loss= 0.0049, Training Accuracy= 0.9992\n",
            "epoch: 80, Training Loss= 0.0057, Training Accuracy= 0.9990\n",
            "epoch: 81, Training Loss= 0.0046, Training Accuracy= 0.9996\n",
            "epoch: 82, Training Loss= 0.0047, Training Accuracy= 0.9993\n",
            "epoch: 83, Training Loss= 0.0047, Training Accuracy= 0.9995\n",
            "epoch: 84, Training Loss= 0.0067, Training Accuracy= 0.9987\n",
            "epoch: 85, Training Loss= 0.0052, Training Accuracy= 0.9993\n",
            "epoch: 86, Training Loss= 0.0026, Training Accuracy= 1.0001\n",
            "epoch: 87, Training Loss= 0.0079, Training Accuracy= 0.9985\n",
            "epoch: 88, Training Loss= 0.0040, Training Accuracy= 0.9996\n",
            "epoch: 89, Training Loss= 0.0061, Training Accuracy= 0.9990\n",
            "epoch: 90, Training Loss= 0.0030, Training Accuracy= 1.0000\n",
            "epoch: 91, Training Loss= 0.0074, Training Accuracy= 0.9987\n",
            "epoch: 92, Training Loss= 0.0039, Training Accuracy= 0.9997\n",
            "epoch: 93, Training Loss= 0.0022, Training Accuracy= 1.0001\n",
            "epoch: 94, Training Loss= 0.0043, Training Accuracy= 0.9995\n",
            "epoch: 95, Training Loss= 0.0035, Training Accuracy= 0.9998\n",
            "epoch: 96, Training Loss= 0.0039, Training Accuracy= 0.9995\n",
            "epoch: 97, Training Loss= 0.0057, Training Accuracy= 0.9990\n",
            "epoch: 98, Training Loss= 0.0047, Training Accuracy= 0.9994\n",
            "epoch: 99, Training Loss= 0.0030, Training Accuracy= 0.9997\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}