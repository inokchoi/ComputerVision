{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inokchoi/ComputerVision/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHASIjy-iotU",
        "outputId": "91f9b350-e47d-487c-eecd-95fdd8425f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from tensorflow.keras.datasets.cifar10 import load_data\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_atx8VliyTS"
      },
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC480SMAi1Ck",
        "outputId": "6efcefca-cf74-4767-d4ed-32fba6931336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# (X_train, y_train), (X_test, y_test) = load_data()\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False, one_hot=True)\n",
        "X_train, y_train = mnist.train.images, mnist.train.labels\n",
        "X_valid, y_valid = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test = mnist.test.images, mnist.test.labels\n",
        "\n",
        "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
        "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "print(\"Validation Set: {} samples\".format(len(X_valid)))\n",
        "print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-a31e29a56efa>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Image Shape: (28, 28, 1)\n",
            "Training Set:   55000 samples\n",
            "Validation Set: 5000 samples\n",
            "Test Set:       10000 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4KJGk9Zi3CY"
      },
      "source": [
        "def conv2d(input, filters, kernel_size, strides=1, activation=tf.nn.relu, padding='SAME', name='conv'):\n",
        "    with tf.variable_scope(name, reuse=False):\n",
        "        out = tf.layers.conv2d(input, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                         activation=activation, name=name)\n",
        "    return out\n",
        "\n",
        "def dense(input, unit, activation=tf.nn.relu, name='dense'):\n",
        "    with tf.variable_scope(name, reuse=False):\n",
        "        out = tf.layers.dense(input, unit, activation=activation, name=name)\n",
        "    return out\n",
        "\n",
        "def max_pool(input, k, s, name='pool'):\n",
        "    out = tf.nn.max_pool(input, ksize=[1, k, k, 1], strides=[1, s, s, 1], padding='SAME', name=name)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g03wiIH_i5fX"
      },
      "source": [
        "def res_block(x, filter_num, kernel_size, strides, name='res_block', downsample=False):\n",
        "    with tf.variable_scope(name, reuse=False):\n",
        "      x_ = x\n",
        "\n",
        "      if downsample:\n",
        "        x_ = conv2d(x_, filter_num, kernel_size, [2,2], name=name)\n",
        "        x = conv2d(x, filter_num, [1,1], [2,2], name='{}_init'.format(name))\n",
        "      else:\n",
        "        x_ = conv2d(x_, filter_num, kernel_size, strides, name=name)\n",
        "      \n",
        "      out = x + x_\n",
        "    return out\n",
        "\n",
        "#resnet-18 layer\n",
        "def res_net(x):\n",
        "    conv1 = conv2d(x, 64, [7,7], [2,2], name='conv1')\n",
        "    \n",
        "    pool1 = max_pool(conv1, 3, 2, name='pool1')\n",
        "\n",
        "    res1 = res_block(pool1, 64, [3,3], [1,1], name='res1_1')\n",
        "    res1 = res_block(res1, 64, [3,3], [1,1], name='res1_2')\n",
        "\n",
        "    res2 = res_block(res1, 128, [3,3], [1,1], name='res2_1', downsample=True)\n",
        "    res2 = res_block(res2, 128, [3,3], [1,1], name='res2_2')\n",
        "    \n",
        "    res3 = res_block(res2, 256, [3,3], [1,1], name='res3_1', downsample=True)\n",
        "    res3 = res_block(res3, 256, [3,3], [1,1], name='res3_2')\n",
        "    \n",
        "    res4 = res_block(res3, 512, [3,3], [1,1], name='res4_1', downsample=True)\n",
        "    res4 = res_block(res4, 512, [3,3], [1,1], name='res4_2')\n",
        "\n",
        "    dense1 = tf.reshape(res4, [-1, tf.size(res4[0])])\n",
        "    dense2 = dense(dense1, 10, name='dense2')\n",
        "    \n",
        "    return dense2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iW4xB7Wi7hT",
        "outputId": "42a0b2ed-34fd-4c7a-e41a-a763e414ef45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.00001\n",
        "batch_size = 64\n",
        "dropout = 0.8\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "y = tf.placeholder(tf.int32, [None, 10])\n",
        "# y_one_hot = tf.squeeze(tf.one_hot(y, 10),axis=1)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32, [])  # dropout (keep probability)\n",
        "x_resize = tf.image.resize_images(x, [64, 64])\n",
        "\n",
        "# Construct model\n",
        "pred = res_net(x)\n",
        "\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.initialize_all_variables()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-58973c5120c5>:4: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "res1_1\n",
            "Tensor(\"res1_1/add:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
            "res1_2\n",
            "Tensor(\"res1_2/add:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
            "res2_1\n",
            "Tensor(\"res2_1/add:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
            "res2_2\n",
            "Tensor(\"res2_2/add:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
            "res3_1\n",
            "Tensor(\"res3_1/add:0\", shape=(?, 2, 2, 256), dtype=float32)\n",
            "res3_2\n",
            "Tensor(\"res3_2/add:0\", shape=(?, 2, 2, 256), dtype=float32)\n",
            "res4_1\n",
            "Tensor(\"res4_1/add:0\", shape=(?, 1, 1, 512), dtype=float32)\n",
            "res4_2\n",
            "Tensor(\"res4_2/add:0\", shape=(?, 1, 1, 512), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-4-58973c5120c5>:9: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV0Mm5iFi99E",
        "outputId": "87923f4d-11e9-4631-a7cb-daffae23bc24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n",
        "                                      gpu_options=tf.GPUOptions(allow_growth=True, visible_device_list='0'))) as sess:\n",
        "    sess.run(init)\n",
        "    # Keep training until reach max iterations\n",
        "    print('Training...')\n",
        "    for i in range(epochs):\n",
        "        s = np.random.permutation(len(X_train))\n",
        "        X_train = X_train[s]\n",
        "        y_train = y_train[s]\n",
        "        loss_total, acc_total = 0, 0\n",
        "        for offset in range(0, len(X_train), batch_size):\n",
        "            end = offset + batch_size\n",
        "            batch_xs, batch_ys = X_train[offset:end], y_train[offset:end]\n",
        "\n",
        "            # Fit training using batch data\n",
        "            _, acc, loss = sess.run([optimizer, accuracy, cost], feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
        "\n",
        "            # Calculate batch accuracy\n",
        "            # Calculate batch loss\n",
        "            acc_total += acc\n",
        "            loss_total += loss\n",
        "\n",
        "        acc_total /= (len(X_train) / batch_size)\n",
        "        loss_total /= (len(X_train) / batch_size)\n",
        "        print(\"epoch: \" + str(i) + \", Training Loss= \" + \"{:.4f}\".format(\n",
        "            loss_total) + \", Training Accuracy= \" + \"{:.4f}\".format(acc_total))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "    saver.save(sess, './resnet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "epoch: 0, Training Loss= 1.2253, Training Accuracy= 0.6531\n",
            "epoch: 1, Training Loss= 0.6879, Training Accuracy= 0.7599\n",
            "epoch: 2, Training Loss= 0.5994, Training Accuracy= 0.7742\n",
            "epoch: 3, Training Loss= 0.5641, Training Accuracy= 0.7812\n",
            "epoch: 4, Training Loss= 0.5455, Training Accuracy= 0.7848\n",
            "epoch: 5, Training Loss= 0.5327, Training Accuracy= 0.7874\n",
            "epoch: 6, Training Loss= 0.5236, Training Accuracy= 0.7894\n",
            "epoch: 7, Training Loss= 0.5160, Training Accuracy= 0.7911\n",
            "epoch: 8, Training Loss= 0.5128, Training Accuracy= 0.7918\n",
            "epoch: 9, Training Loss= 0.5061, Training Accuracy= 0.7939\n",
            "epoch: 10, Training Loss= 0.5025, Training Accuracy= 0.7944\n",
            "epoch: 11, Training Loss= 0.4990, Training Accuracy= 0.7954\n",
            "epoch: 12, Training Loss= 0.4965, Training Accuracy= 0.7957\n",
            "epoch: 13, Training Loss= 0.4933, Training Accuracy= 0.7969\n",
            "epoch: 14, Training Loss= 0.4905, Training Accuracy= 0.7973\n",
            "epoch: 15, Training Loss= 0.4893, Training Accuracy= 0.7976\n",
            "epoch: 16, Training Loss= 0.3243, Training Accuracy= 0.8751\n",
            "epoch: 17, Training Loss= 0.2876, Training Accuracy= 0.8868\n",
            "epoch: 18, Training Loss= 0.2832, Training Accuracy= 0.8874\n",
            "epoch: 19, Training Loss= 0.2792, Training Accuracy= 0.8881\n",
            "epoch: 20, Training Loss= 0.2770, Training Accuracy= 0.8884\n",
            "epoch: 21, Training Loss= 0.2754, Training Accuracy= 0.8884\n",
            "epoch: 22, Training Loss= 0.2735, Training Accuracy= 0.8890\n",
            "epoch: 23, Training Loss= 0.2711, Training Accuracy= 0.8900\n",
            "epoch: 24, Training Loss= 0.2706, Training Accuracy= 0.8896\n",
            "epoch: 25, Training Loss= 0.2678, Training Accuracy= 0.8900\n",
            "epoch: 26, Training Loss= 0.2661, Training Accuracy= 0.8906\n",
            "epoch: 27, Training Loss= 0.2656, Training Accuracy= 0.8908\n",
            "epoch: 28, Training Loss= 0.2637, Training Accuracy= 0.8915\n",
            "epoch: 29, Training Loss= 0.2629, Training Accuracy= 0.8917\n",
            "epoch: 30, Training Loss= 0.2616, Training Accuracy= 0.8919\n",
            "epoch: 31, Training Loss= 0.2611, Training Accuracy= 0.8922\n",
            "epoch: 32, Training Loss= 0.2603, Training Accuracy= 0.8922\n",
            "epoch: 33, Training Loss= 0.2592, Training Accuracy= 0.8927\n",
            "epoch: 34, Training Loss= 0.2590, Training Accuracy= 0.8924\n",
            "epoch: 35, Training Loss= 0.2568, Training Accuracy= 0.8931\n",
            "epoch: 36, Training Loss= 0.2569, Training Accuracy= 0.8928\n",
            "epoch: 37, Training Loss= 0.2554, Training Accuracy= 0.8935\n",
            "epoch: 38, Training Loss= 0.2546, Training Accuracy= 0.8936\n",
            "epoch: 39, Training Loss= 0.2542, Training Accuracy= 0.8934\n",
            "epoch: 40, Training Loss= 0.2536, Training Accuracy= 0.8938\n",
            "epoch: 41, Training Loss= 0.2531, Training Accuracy= 0.8937\n",
            "epoch: 42, Training Loss= 0.2525, Training Accuracy= 0.8942\n",
            "epoch: 43, Training Loss= 0.2521, Training Accuracy= 0.8942\n",
            "epoch: 44, Training Loss= 0.2510, Training Accuracy= 0.8944\n",
            "epoch: 45, Training Loss= 0.2510, Training Accuracy= 0.8943\n",
            "epoch: 46, Training Loss= 0.2501, Training Accuracy= 0.8950\n",
            "epoch: 47, Training Loss= 0.2493, Training Accuracy= 0.8951\n",
            "epoch: 48, Training Loss= 0.2491, Training Accuracy= 0.8952\n",
            "epoch: 49, Training Loss= 0.2488, Training Accuracy= 0.8950\n",
            "epoch: 50, Training Loss= 0.2483, Training Accuracy= 0.8950\n",
            "epoch: 51, Training Loss= 0.2474, Training Accuracy= 0.8955\n",
            "epoch: 52, Training Loss= 0.2479, Training Accuracy= 0.8951\n",
            "epoch: 53, Training Loss= 0.2467, Training Accuracy= 0.8956\n",
            "epoch: 54, Training Loss= 0.2462, Training Accuracy= 0.8958\n",
            "epoch: 55, Training Loss= 0.2468, Training Accuracy= 0.8954\n",
            "epoch: 56, Training Loss= 0.2457, Training Accuracy= 0.8959\n",
            "epoch: 57, Training Loss= 0.2451, Training Accuracy= 0.8961\n",
            "epoch: 58, Training Loss= 0.2450, Training Accuracy= 0.8961\n",
            "epoch: 59, Training Loss= 0.2447, Training Accuracy= 0.8961\n",
            "epoch: 60, Training Loss= 0.2439, Training Accuracy= 0.8962\n",
            "epoch: 61, Training Loss= 0.2438, Training Accuracy= 0.8965\n",
            "epoch: 62, Training Loss= 0.2440, Training Accuracy= 0.8962\n",
            "epoch: 63, Training Loss= 0.2436, Training Accuracy= 0.8966\n",
            "epoch: 64, Training Loss= 0.2436, Training Accuracy= 0.8964\n",
            "epoch: 65, Training Loss= 0.2430, Training Accuracy= 0.8965\n",
            "epoch: 66, Training Loss= 0.2436, Training Accuracy= 0.8964\n",
            "epoch: 67, Training Loss= 0.2424, Training Accuracy= 0.8967\n",
            "epoch: 68, Training Loss= 0.2426, Training Accuracy= 0.8965\n",
            "epoch: 69, Training Loss= 0.2420, Training Accuracy= 0.8968\n",
            "epoch: 70, Training Loss= 0.2421, Training Accuracy= 0.8966\n",
            "epoch: 71, Training Loss= 0.2419, Training Accuracy= 0.8968\n",
            "epoch: 72, Training Loss= 0.2417, Training Accuracy= 0.8968\n",
            "epoch: 73, Training Loss= 0.2422, Training Accuracy= 0.8966\n",
            "epoch: 74, Training Loss= 0.2408, Training Accuracy= 0.8970\n",
            "epoch: 75, Training Loss= 0.2416, Training Accuracy= 0.8970\n",
            "epoch: 76, Training Loss= 0.2404, Training Accuracy= 0.8972\n",
            "epoch: 77, Training Loss= 0.2407, Training Accuracy= 0.8971\n",
            "epoch: 78, Training Loss= 0.2404, Training Accuracy= 0.8973\n",
            "epoch: 79, Training Loss= 0.2407, Training Accuracy= 0.8971\n",
            "epoch: 80, Training Loss= 0.2406, Training Accuracy= 0.8972\n",
            "epoch: 81, Training Loss= 0.2402, Training Accuracy= 0.8972\n",
            "epoch: 82, Training Loss= 0.2396, Training Accuracy= 0.8975\n",
            "epoch: 83, Training Loss= 0.2402, Training Accuracy= 0.8971\n",
            "epoch: 84, Training Loss= 0.2406, Training Accuracy= 0.8970\n",
            "epoch: 85, Training Loss= 0.2395, Training Accuracy= 0.8973\n",
            "epoch: 86, Training Loss= 0.2393, Training Accuracy= 0.8976\n",
            "epoch: 87, Training Loss= 0.2398, Training Accuracy= 0.8972\n",
            "epoch: 88, Training Loss= 0.2389, Training Accuracy= 0.8976\n",
            "epoch: 89, Training Loss= 0.2399, Training Accuracy= 0.8973\n",
            "epoch: 90, Training Loss= 0.2389, Training Accuracy= 0.8976\n",
            "epoch: 91, Training Loss= 0.2389, Training Accuracy= 0.8976\n",
            "epoch: 92, Training Loss= 0.2387, Training Accuracy= 0.8976\n",
            "epoch: 93, Training Loss= 0.2396, Training Accuracy= 0.8974\n",
            "epoch: 94, Training Loss= 0.2387, Training Accuracy= 0.8977\n",
            "epoch: 95, Training Loss= 0.2391, Training Accuracy= 0.8975\n",
            "epoch: 96, Training Loss= 0.2397, Training Accuracy= 0.8973\n",
            "epoch: 97, Training Loss= 0.2384, Training Accuracy= 0.8976\n",
            "epoch: 98, Training Loss= 0.2378, Training Accuracy= 0.8978\n",
            "epoch: 99, Training Loss= 0.2407, Training Accuracy= 0.8970\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}